# -*- coding: utf-8 -*-
"""Self Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TeJQgDSHkF7F4f4GqZUbZndyNKMQVdjB
"""

!pip install pandas numpy matplotlib seaborn geopy scikit-learn xgboost catboost ngboost shap folium ortools

import pandas as pd
import numpy as np
import random
from datetime import datetime, timedelta

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

# Number of synthetic orders
n_orders = 10000

# Define depot location (center point, e.g., in Delhi)
depot_lat, depot_lon = 28.6139, 77.2090

# Generate order times across 7 days
start_date = datetime(2023, 1, 1)
order_times = [start_date + timedelta(minutes=random.randint(0, 7*24*60)) for _ in range(n_orders)]

# Generate delivery coordinates within ~10 km radius
def generate_random_location(center_lat, center_lon, radius_km=10):
    # Approx 1 degree latitude ~= 111 km
    radius_deg = radius_km / 111
    lat_offset = np.random.uniform(-radius_deg, radius_deg)
    lon_offset = np.random.uniform(-radius_deg, radius_deg)
    return center_lat + lat_offset, center_lon + lon_offset

delivery_coords = [generate_random_location(depot_lat, depot_lon) for _ in range(n_orders)]
delivery_lat = [coord[0] for coord in delivery_coords]
delivery_lon = [coord[1] for coord in delivery_coords]

# Assign delivery windows (1–2 hours after order)
delivery_window_start = [order + timedelta(minutes=random.randint(60, 90)) for order in order_times]
delivery_window_end = [start + timedelta(minutes=random.randint(30, 60)) for start in delivery_window_start]

# Assign other features
vehicle_ids = [random.randint(1, 20) for _ in range(n_orders)]
package_weights = np.round(np.random.uniform(0.5, 15.0, n_orders), 2)
priority_levels = np.random.choice(['High', 'Medium', 'Low'], n_orders, p=[0.2, 0.5, 0.3])

# Approximate distance from depot (using Euclidean for simplicity)
distance_km = [np.sqrt((lat - depot_lat)**2 + (lon - depot_lon)**2) * 111 for lat, lon in zip(delivery_lat, delivery_lon)]

# Assemble dataframe
df = pd.DataFrame({
    'order_time': order_times,
    'delivery_window_start': delivery_window_start,
    'delivery_window_end': delivery_window_end,
    'delivery_lat': delivery_lat,
    'delivery_lon': delivery_lon,
    'vehicle_id': vehicle_ids,
    'package_weight_kg': package_weights,
    'priority': priority_levels,
    'distance_km': np.round(distance_km, 2)
})

# Preview
print(df.head())

import numpy as np
import pandas as pd
from datetime import timedelta

# Set random seed for reproducibility
np.random.seed(42)

# 1. GPS Noise (lat/lon jitter within ~50m)
df['delivery_lat'] += np.random.normal(0, 0.0005, size=len(df))
df['delivery_lon'] += np.random.normal(0, 0.0005, size=len(df))

# 2. Distance Noise (~±5%)
df['distance_km'] *= np.random.normal(1.0, 0.05, size=len(df))
df['distance_km'] = df['distance_km'].clip(lower=0.1)  # prevent negative/zero distances

# 3. Package Weight Noise (~±10%)
df['package_weight_kg'] *= np.random.normal(1.0, 0.1, size=len(df))
df['package_weight_kg'] = df['package_weight_kg'].clip(lower=0.1)

# 4. Time Window Jitter (±15 minutes)
def jitter_time(ts):
    jitter = timedelta(minutes=np.random.randint(-15, 16))
    return ts + jitter

df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start']).apply(jitter_time)
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end']).apply(jitter_time)

# Ensure end > start
df['delivery_window_end'] = np.maximum(df['delivery_window_end'], df['delivery_window_start'] + timedelta(minutes=10))

# 5. Priority Flipping (~2% of rows)
flip_mask = np.random.rand(len(df)) < 0.02
df.loc[flip_mask, 'priority'] = df.loc[flip_mask, 'priority'].replace({'Low': 'Medium', 'Medium': 'Low'})

# 6. Vehicle Reassignment (~1%)
unique_vehicles = df['vehicle_id'].unique()
reassign_mask = np.random.rand(len(df)) < 0.01
df.loc[reassign_mask, 'vehicle_id'] = np.random.choice(unique_vehicles, size=reassign_mask.sum())

# Final sanity checks
df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start'])
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end'])

print("✅ Noise added to dataset successfully.")

import numpy as np
import pandas as pd

# If not already datetime, convert to datetime
df['order_time'] = pd.to_datetime(df['order_time'])
df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start'])
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end'])

# 1. Add traffic level
traffic_levels = ['Low', 'Medium', 'High']
df['traffic_level'] = np.random.choice(traffic_levels, size=len(df), p=[0.4, 0.4, 0.2])

# 2. Add weather condition
weather_conditions = ['Clear', 'Rain', 'Snow', 'Fog']
df['weather_condition'] = np.random.choice(weather_conditions, size=len(df), p=[0.6, 0.2, 0.1, 0.1])

# 3. Add vehicle type
vehicle_types = ['EV', 'GDV']
df['vehicle_type'] = np.random.choice(vehicle_types, size=len(df), p=[0.5, 0.5])

# 4. Add number of stops before delivery
df['num_stops_before'] = np.random.randint(0, 6, size=len(df))

# 5. Add target column: route_duration_min
base_speed_kmph = 30
speed_modifiers = {
    'Low': 1.0,
    'Medium': 0.85,
    'High': 0.7
}
weather_delays = {
    'Clear': 0,
    'Rain': 5,
    'Snow': 10,
    'Fog': 7
}

# Calculate speed based on traffic
df['speed_kmph'] = df['traffic_level'].map(speed_modifiers) * base_speed_kmph

# Base route duration
df['route_duration_min'] = (df['distance_km'] / df['speed_kmph']) * 60

# Add delays
df['route_duration_min'] += df['weather_condition'].map(weather_delays)
df['route_duration_min'] += df['num_stops_before'] * np.random.uniform(2, 5, size=len(df))

# Add small Gaussian noise
df['route_duration_min'] += np.random.normal(loc=0, scale=2, size=len(df))

# Ensure no negative durations
df['route_duration_min'] = df['route_duration_min'].clip(lower=10).round(2)

# Drop speed_kmph if not needed
df.drop(columns=['speed_kmph'], inplace=True)

# Preview updated DataFrame
print(df.head())

import numpy as np

def inject_realism(df, seed=42):
    np.random.seed(seed)

    # 1. Add Gaussian noise to target
    df['delivery_duration_mins'] += np.random.normal(loc=0, scale=5, size=len(df))
    df['delivery_duration_mins'] = df['delivery_duration_mins'].clip(lower=10, upper=90)

    # 2. Add noise to key correlated features
    df['route_duration_min'] += np.random.normal(0, 3, len(df))
    df['urgency_mins'] += np.random.normal(0, 2, len(df))
    df['duration_minutes'] += np.random.normal(0, 2.5, len(df))

    # 3. Inject minor logical inconsistency
    idx_urgency = df.sample(frac=0.05, random_state=seed).index
    df.loc[idx_urgency, 'urgency_mins'] = df['urgency_mins'].sample(frac=0.05, random_state=seed+1).values

    # 4. Mismatch in categorical-label vs actual value
    idx_weather = df[(df['weather_condition'] == 'Clear') & (df['delivery_duration_mins'] > 70)].index
    df.loc[idx_weather, 'weather_condition'] = 'Rain'

    idx_traffic = df.sample(frac=0.03, random_state=seed).index
    df.loc[idx_traffic, 'traffic_level'] = np.random.choice(['High', 'Medium', 'Low'], size=len(idx_traffic))

    # 5. Ensure target has no extreme outliers
    df['delivery_duration_mins'] = df['delivery_duration_mins'].clip(lower=10, upper=90)

    return df

"""#Data Cleaning and Preprocessing"""

# Check for NaNs (due to noise injection or bugs)
print(df.isna().sum())

# Check duration logic
df['delivery_duration_mins'] = (df['delivery_window_end'] - df['delivery_window_start']).dt.total_seconds() / 60
print("Min duration:", df['delivery_duration_mins'].min())
print("Negative durations:", (df['delivery_duration_mins'] < 0).sum())

# Clip invalid durations (optional safety net)
df = df[df['delivery_duration_mins'] > 0]

df['order_time'] = pd.to_datetime(df['order_time'])
df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start'])
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end'])

df = df[df['route_duration_min'] > 0]

print("Latitude range:", df['delivery_lat'].min(), "to", df['delivery_lat'].max())
print("Longitude range:", df['delivery_lon'].min(), "to", df['delivery_lon'].max())

# Optional: Add a safety buffer (±0.01) if you like
df = df[
    df['delivery_lat'].between(28.52, 28.71) &
    df['delivery_lon'].between(77.11, 77.31)
]

# Keep necessary cleanup
df['order_time'] = pd.to_datetime(df['order_time'])
df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start'])
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end'])

# Duration should be positive
df['duration_minutes'] = (df['delivery_window_end'] - df['order_time']).dt.total_seconds() / 60
df = df[df['duration_minutes'] > 0]

df.info()

# prompt: check if delivery_duration_mins and duration_minutes are identical by row or not

# Compare the two columns row by row
comparison_result = df['delivery_duration_mins'] == df['duration_minutes']

# Count how many rows are identical
identical_rows_count = comparison_result.sum()

# Count how many rows are different
different_rows_count = len(df) - identical_rows_count

print(f"Number of rows where 'delivery_duration_mins' and 'duration_minutes' are identical: {identical_rows_count}")
print(f"Number of rows where 'delivery_duration_mins' and 'duration_minutes' are different: {different_rows_count}")

# Optionally, display the first few rows where they differ
if different_rows_count > 0:
  print("\nSample rows where 'delivery_duration_mins' and 'duration_minutes' differ:")
  print(df[comparison_result == False][['delivery_duration_mins', 'duration_minutes']].head())

# Note: Based on the data generation logic, 'delivery_duration_mins' is the difference between
# 'delivery_window_end' and 'delivery_window_start', while 'duration_minutes' is the difference
# between 'delivery_window_end' and 'order_time'. These are inherently different calculations,
# so they are not expected to be identical. The comparison above confirms this.

"""#Feature Engineering"""

import pandas as pd
import numpy as np

# Assuming your dataset is in 'df'
df['order_time'] = pd.to_datetime(df['order_time'])
df['delivery_window_start'] = pd.to_datetime(df['delivery_window_start'])
df['delivery_window_end'] = pd.to_datetime(df['delivery_window_end'])

# Hour of the day
df['order_hour'] = df['order_time'].dt.hour

# Day of the week
df['order_dayofweek'] = df['order_time'].dt.dayofweek  # 0 = Monday

# Delivery window duration in minutes
df['delivery_window_duration_mins'] = (df['delivery_window_end'] - df['delivery_window_start']).dt.total_seconds() / 60

# Urgency: time between order time and delivery end
df['urgency_mins'] = (df['delivery_window_end'] - df['order_time']).dt.total_seconds() / 60

# Weight category
df['weight_category'] = pd.cut(df['package_weight_kg'],
                                bins=[0, 1, 5, 10, np.inf],
                                labels=['Very Light', 'Light', 'Medium', 'Heavy'])

# Example categorical encoding for priority (if not already encoded)
priority_map = {'Low': 0, 'Medium': 1, 'High': 2}
df['priority_level'] = df['priority'].map(priority_map)

df = inject_realism(df)

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Step 1: Extract relevant coordinates
coords = df[['delivery_lat', 'delivery_lon']]

# Step 2: Normalize coordinates to avoid bias in clustering
scaler = StandardScaler()
coords_scaled = scaler.fit_transform(coords)

# Step 3: Apply KMeans clustering
kmeans = KMeans(n_clusters=10, random_state=42, n_init='auto')  # You can adjust n_clusters
df['location_cluster'] = kmeans.fit_predict(coords_scaled)

import matplotlib.pyplot as plt

# Set up the plot
plt.figure(figsize=(10, 8))
scatter = plt.scatter(
    df['delivery_lon'],
    df['delivery_lat'],
    c=df['location_cluster'],
    cmap='tab10',
    alpha=0.6,
    s=30
)

# Add legend, labels, and title
plt.colorbar(scatter, label='Cluster ID')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.title('Delivery Location Clusters')
plt.grid(True)
plt.show()

from sklearn.metrics.pairwise import haversine_distances
import numpy as np

# Convert lat/lon to radians
coords_rad = np.radians(df[['delivery_lat', 'delivery_lon']].values)

# Compute cluster centroids (mean of lat/lon for each cluster)
cluster_centroids = df.groupby('location_cluster')[['delivery_lat', 'delivery_lon']].mean()
centroids_rad = np.radians(cluster_centroids.values)

# Map cluster_id to corresponding centroid
cluster_to_centroid = dict(zip(cluster_centroids.index, centroids_rad))

# Calculate haversine distance to cluster centroid for each row
df['distance_to_cluster_center_km'] = df.apply(
    lambda row: haversine_distances(
        [coords_rad[row.name]],
        [cluster_to_centroid[row['location_cluster']]]
    )[0][0] * 6371,  # Earth radius in km
    axis=1
)

# prompt: compare the columns "delivery_window_duration_mins" and "delivery_duration_mins"...I mean to say comparison as in whether their contents are exactly same or not..if they are,remove one of the columns

# Compare the columns "delivery_window_duration_mins" and "delivery_duration_mins"
# Check if the contents are exactly the same
are_columns_same = df['delivery_window_duration_mins'].equals(df['delivery_duration_mins'])

if are_columns_same:
  print("Columns 'delivery_window_duration_mins' and 'delivery_duration_mins' are identical. Removing one.")
  # Remove one of the columns (e.g., 'delivery_duration_mins')
  df = df.drop(columns=['delivery_window_duration_mins'])
else:
  print("Columns 'delivery_window_duration_mins' and 'delivery_duration_mins' are not identical.")

import pandas as pd
import holidays

# Weekend flag
df['is_weekend'] = df['order_time'].dt.dayofweek >= 5
df['is_weekend'] = df['is_weekend'].astype(int)

# Holiday flag using Indian holidays
indian_holidays = holidays.India(years=df['order_time'].dt.year.unique())
df['is_holiday'] = df['order_time'].dt.date.astype('datetime64[ns]').isin(indian_holidays)
df['is_holiday'] = df['is_holiday'].astype(int)

# List of unique holiday dates in your data
holiday_dates = df[df['is_holiday'] == 1]['order_time'].dt.date.unique()
print("Holiday Dates in Data:", holiday_dates)

# Check if any holiday is on a weekend
for d in holiday_dates:
    dow = pd.to_datetime(d).dayofweek
    if dow >= 5:
        print(f"{d} is on a weekend!")

# prompt: one hot encoding led to dropping of the intial columns,cant I modify the code to keep the original columns while adding the new columns

import pandas as pd
# One-hot encode categorical features, keeping original columns
categorical_cols = ['traffic_level', 'vehicle_type', 'weather_condition']

# Use pd.get_dummies and concatenate with the original dataframe
df = pd.concat([df, pd.get_dummies(df[categorical_cols], prefix=categorical_cols, drop_first=False)], axis=1)

# prompt: label and one hot encoding in weight category

import pandas as pd
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Label Encoding for weight_category
label_encoder = LabelEncoder()
df['weight_category_encoded_label'] = label_encoder.fit_transform(df['weight_category'])

# One-Hot Encoding for weight_category
# pd.get_dummies is often simpler for one-hot encoding directly
df = pd.concat([df, pd.get_dummies(df['weight_category'], prefix='weight_category_onehot', drop_first=False)], axis=1)

# Print the first few rows to see the new columns
print(df[['package_weight_kg', 'weight_category', 'weight_category_encoded_label', 'weight_category_onehot_Very Light', 'weight_category_onehot_Light', 'weight_category_onehot_Medium', 'weight_category_onehot_Heavy']].head())

df.info()

from sklearn.preprocessing import MinMaxScaler

# Define columns to normalize
numerical_cols = [
    'package_weight_kg',
    'distance_km',
    'route_duration_min',
    'delivery_duration_mins',
    'duration_minutes',
    'order_hour',
    'order_dayofweek',
    'urgency_mins',
    'distance_to_cluster_center_km',
    'num_stops_before'
]

# Initialize the scaler
scaler = MinMaxScaler()

# Fit and transform the numerical columns
df_scaled = scaler.fit_transform(df[numerical_cols])

# Convert back to DataFrame
df_scaled = pd.DataFrame(df_scaled, columns=[col + '_scaled' for col in numerical_cols])

# Concatenate back with original DataFrame (keeping both raw and scaled versions)
df = pd.concat([df, df_scaled], axis=1)

# Interaction Feature 1: Urgency × Distance
df['urgency_distance_interaction'] = df['urgency_mins'] * df['distance_km']

# Interaction Feature 2: Weight × Stops
df['weight_stops_interaction'] = df['package_weight_kg'] * df['num_stops_before']

# Interaction Feature 3: Priority Level × Distance
df['priority_distance_interaction'] = df['priority_level'] * df['distance_km']

# Interaction Feature 4: Location Cluster × Route Duration
df['cluster_route_interaction'] = df['location_cluster'] * df['route_duration_min']

# Group by cluster
cluster_stats = df.groupby('location_cluster').agg({
    'duration_minutes': 'mean',
    'urgency_mins': 'mean',
    'package_weight_kg': 'mean',
    'num_stops_before': 'mean'
}).rename(columns={
    'duration_minutes': 'cluster_avg_duration',
    'urgency_mins': 'cluster_avg_urgency',
    'package_weight_kg': 'cluster_avg_weight',
    'num_stops_before': 'cluster_avg_num_stops'
}).reset_index()

# Merge back to original DataFrame
df = df.merge(cluster_stats, on='location_cluster', how='left')

import numpy as np
import pandas as pd

# Binning: Distance
df['distance_bin'] = pd.cut(df['distance_km'],
                            bins=[0, 2, 5, 10, 20, 50, np.inf],
                            labels=['0–2km', '2–5km', '5–10km', '10–20km', '20–50km', '50km+'])

# Binning: Delivery Duration
df['duration_bin'] = pd.cut(df['duration_minutes'],
                            bins=[0, 15, 30, 60, 90, 120, np.inf],
                            labels=['0–15m', '15–30m', '30–60m', '60–90m', '90–120m', '120m+'])

# Binning: Urgency
df['urgency_bin'] = pd.cut(df['urgency_mins'],
                           bins=[-np.inf, 0, 30, 60, 120, np.inf],
                           labels=['Overdue', '0–30m', '30–60m', '60–120m', '120m+'])

df.info()

"""#Exploratory Data Analysis"""

print(df.shape)
print(df.dtypes)
print(df.describe())
print(df.isnull().sum())

"""1.Target Variable Distribution"""

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.histplot(df['delivery_duration_mins'], bins=50, kde=True)
plt.title("Distribution of Delivery Duration (Minutes)")
plt.xlabel("Delivery Duration")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
sns.histplot(df['duration_minutes'], bins=50, kde=True)
plt.title("Distribution of Total Duration (Minutes)")
plt.xlabel("Total Duration")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

"""1.1 Outlier Detection(Boxplot)"""

plt.figure(figsize=(8, 4))
sns.boxplot(x=df['delivery_duration_mins'], color='salmon')
plt.title('Boxplot of Delivery Duration')
plt.xlabel('Delivery Duration (mins)')
plt.grid(True)
plt.tight_layout()
plt.show()

# prompt: boxplot of priority

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
sns.boxplot(x=df['priority'], color='lightblue')
plt.title('Boxplot of Priority')
plt.xlabel('Priority')
plt.grid(True)
plt.tight_layout()
plt.show()

# prompt: boxplot of vehicle_type

import matplotlib.pyplot as plt
plt.figure(figsize=(8, 4))
sns.boxplot(x=df['vehicle_type'], color='lightgreen')
plt.title('Boxplot of Vehicle Type')
plt.xlabel('Vehicle Type')
plt.grid(True)
plt.tight_layout()
plt.show()

"""1.2 Statistical Checks"""

# Skewness & Kurtosis
from scipy.stats import skew, kurtosis

delivery_skew = skew(df['delivery_duration_mins'])
delivery_kurt = kurtosis(df['delivery_duration_mins'])

print(f"Skewness: {delivery_skew:.2f}")
print(f"Kurtosis: {delivery_kurt:.2f}")

# Null Checks
print("\nMissing values:\n", df['delivery_duration_mins'].isnull().sum())

# Descriptive Statistics
print("\nDescriptive stats:")
print(df['delivery_duration_mins'].describe())

"""1.3 Sanity Check: Delivery Duration vs Route Duration"""

# Check if delivery duration is consistently greater than route duration
df['duration_diff'] = df['delivery_duration_mins'] - df['route_duration_min']

plt.figure(figsize=(10,6))
sns.histplot(df['duration_diff'], bins=50, kde=True, color='lightgreen', edgecolor='black')
plt.title('Delivery Duration minus Route Duration')
plt.xlabel('Duration Difference (mins)')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

# Check for any illogical cases (negative durations)
illogical_cases = df[df['duration_diff'] < 0]
print(f"\nNumber of cases where delivery duration < route duration: {len(illogical_cases)}")

df['faster_than_expected'] = df['delivery_duration_mins'] < df['route_duration_min']
df['duration_diff'] = df['delivery_duration_mins'] - df['route_duration_min']

"""1.4 Correlation with Target Variable"""

# Correlation with numeric features only
numeric_features = df.select_dtypes(include=['int64', 'float64']).drop(columns=['delivery_duration_mins'])
corrs = numeric_features.corrwith(df['delivery_duration_mins']).sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=corrs.values, y=corrs.index, palette='viridis')
plt.title('Correlation of Features with Delivery Duration')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Feature')
plt.grid(True)
plt.tight_layout()
plt.show()

"""1.5 Delivery Duration vs Distance"""

plt.figure(figsize=(8,6))
sns.scatterplot(data=df, x='distance_km', y='delivery_duration_mins', alpha=0.3)
plt.title('Delivery Duration vs Distance')
plt.xlabel('Distance (km)')
plt.ylabel('Delivery Duration (mins)')
plt.grid(True)
plt.show()

"""1.6 Delivery Duration by Hour of Day"""

plt.figure(figsize=(10,6))
sns.boxplot(data=df, x='order_hour', y='delivery_duration_mins')
plt.title('Delivery Duration by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Delivery Duration (mins)')
plt.grid(True)
plt.show()

"""1.7 Delivery Duration by Day of Week"""

plt.figure(figsize=(10,6))
sns.boxplot(data=df, x='order_dayofweek', y='delivery_duration_mins')
plt.title('Delivery Duration by Day of Week')
plt.xlabel('Day of Week (0=Mon)')
plt.ylabel('Delivery Duration (mins)')
plt.grid(True)
plt.show()

"""1.8 Distribution of Duration Differences for Early vs Late Deliveries"""

plt.figure(figsize=(10,6))
sns.histplot(df[df['duration_diff'] < 0]['duration_diff'], bins=30, kde=True, color='blue', label='Early Deliveries')
sns.histplot(df[df['duration_diff'] > 0]['duration_diff'], bins=30, kde=True, color='red', label='Late Deliveries')
plt.title('Distribution of Delivery-Route Duration Differences')
plt.xlabel('Duration Difference (mins)')
plt.ylabel('Frequency')
plt.legend()
plt.grid(True)
plt.show()

"""1.9 Top Correlated Features"""

top_corr_features = df.corr(numeric_only=True)['delivery_duration_mins'].abs().sort_values(ascending=False).head(10).index
plt.figure(figsize=(10,8))
sns.heatmap(df[top_corr_features].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Top Features')
plt.show()

# prompt: I want a list of all correlated features along with correlation values with respect to delivery_duration_mins but arranged in desceding order

import matplotlib.pyplot as plt
# Calculate correlations with 'delivery_duration_mins'
correlations = df.corr(numeric_only=True)['delivery_duration_mins']

# Drop the correlation of the target variable with itself
correlations = correlations.drop('delivery_duration_mins')

# Sort the correlations in descending order by absolute value (for strongest relationships)
# Or by the value itself to see positive/negative relationships directly
# For descending order of correlation value:
sorted_correlations = correlations.sort_values(ascending=False)

# Print the list of features and their correlations
print("Correlation with delivery_duration_mins (Descending Order):")
print(sorted_correlations)

# Optional: Plotting the sorted correlations
plt.figure(figsize=(10, 8))
sns.barplot(x=sorted_correlations.values, y=sorted_correlations.index, palette='coolwarm')
plt.title('Correlation of Features with Delivery Duration (Sorted)')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Feature')
plt.grid(axis='x')
plt.tight_layout()
plt.show()

"""1.10 Plotly Scatter MapBox"""

import plotly.express as px

# If you don't already have a token, use public Mapbox style
px.set_mapbox_access_token("pk.eyJ1IjoiZGVtbyIsImEiOiJja2xrbG9vbGswZ3R4MnZwZ2lwYjQweXAxIn0.H0EOXqHk0_NMfU-2_mq-YQ")

fig = px.scatter_mapbox(
    df,
    lat="delivery_lat",
    lon="delivery_lon",
    color="location_cluster",
    hover_data=["vehicle_id", "priority", "distance_km"],
    zoom=10,
    height=600,
    title="Geospatial Clustering of Delivery Locations"
)

fig.show()

import folium
from folium.plugins import MarkerCluster
import pandas as pd

# Center map around the average location
map_center = [df['delivery_lat'].mean(), df['delivery_lon'].mean()]
m = folium.Map(location=map_center, zoom_start=11)

# Create marker clusters
marker_cluster = MarkerCluster().add_to(m)

# Add points to the map
for _, row in df.iterrows():
    folium.CircleMarker(
        location=[row['delivery_lat'], row['delivery_lon']],
        radius=3,
        popup=f"Cluster: {row['location_cluster']}",
        color=f"hsl({(row['location_cluster'] * 47) % 360}, 70%, 50%)",  # Generates color by cluster
        fill=True,
        fill_opacity=0.7
    ).add_to(marker_cluster)

# Save to HTML
m.save("geospatial_clusters_map.html")

"""2. Correlation Heatmap"""

numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
plt.figure(figsize=(14, 10))
sns.heatmap(df[numerical_cols].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Matrix (Numerical Features)")
plt.show()

"""3.1 Traffic and Weather Effects"""

# Traffic
plt.figure(figsize=(10, 5))
sns.boxplot(x='traffic_level', y='delivery_duration_mins', data=df)
plt.title("Delivery Duration by Traffic Level")
plt.grid(True)
plt.show()

# Weather
plt.figure(figsize=(10, 5))
sns.boxplot(x='weather_condition', y='delivery_duration_mins', data=df)
plt.title("Delivery Duration by Weather Condition")
plt.grid(True)
plt.show()

"""4. Package Weight and Stops Effect"""

sns.boxplot(x='weight_category', y='delivery_duration_mins', data=df)
plt.title("Delivery Duration by Package Weight Category")
plt.show()

plt.figure(figsize=(10, 5))
sns.scatterplot(x='package_weight_kg', y='delivery_duration_mins', hue='priority_level', alpha=0.5, data=df)
plt.title("Delivery Duration vs Package Weight")
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(x='num_stops_before', y='delivery_duration_mins', data=df)
plt.title("Delivery Duration vs Number of Stops Before")
plt.grid(True)
plt.show()

"""5. Spatial Distribution"""

plt.figure(figsize=(8, 6))
sns.scatterplot(x='delivery_lon', y='delivery_lat', hue='location_cluster', palette='tab10', alpha=0.6, data=df)
plt.title("Delivery Locations Colored by Cluster")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.grid(True)
plt.show()

"""6. Residual Style Analysis"""

# Example: Delivery duration vs. urgency
sns.scatterplot(data=df, x='urgency_mins', y='delivery_duration_mins', hue='priority', alpha=0.6)
plt.title("Delivery Duration vs. Urgency Minutes")
plt.show()

"""7.Delivery Delays Relative to Time Window"""

df['late_by_minutes'] = (df['delivery_window_end'] - df['order_time']).dt.total_seconds()/60 - df['delivery_duration_mins']

sns.histplot(df['late_by_minutes'], bins=30, kde=True)
plt.title("How Early or Late Deliveries Tend to Be")
plt.xlabel("Minutes Left in Window After Delivery")
plt.show()

"""8.Delivery Status Category"""

df['delivery_status'] = df['delivery_duration_mins'] - df['route_duration_min']
df['delivery_status_bucket'] = pd.cut(
    df['delivery_status'],
    bins=[-float('inf'), -5, 5, 15, float('inf')],
    labels=['Early (>5min)', 'On-Time (±5min)', 'Slightly Late (5-15min)', 'Very Late (>15min)']
)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(8,5))
sns.countplot(x='delivery_status_bucket', data=df, order=['Early (>5min)', 'On-Time (±5min)', 'Slightly Late (5-15min)', 'Very Late (>15min)'])
plt.title('Delivery Punctuality Buckets')
plt.ylabel('Number of Deliveries')
plt.xlabel('Delivery Status')
plt.xticks(rotation=15)
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""9.ClusterWise Average Delivery Duration"""

cluster_avg_duration = df.groupby('location_cluster')['delivery_duration_mins'].mean().reset_index()

plt.figure(figsize=(8,5))
sns.barplot(data=cluster_avg_duration, x='location_cluster', y='delivery_duration_mins', palette='viridis')
plt.title('Average Delivery Duration by Geospatial Cluster')
plt.xlabel('Location Cluster')
plt.ylabel('Avg Delivery Duration (min)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""10.Vehicle Level Delivery Duration Spread"""

# prompt: cant the above graph be sorted in order of vehicle id?

import matplotlib.pyplot as plt
# 10.Vehicle Level Delivery Duration Spread - Sorted by Vehicle ID
vehicle_avg_duration = df.groupby('vehicle_id')['delivery_duration_mins'].mean().sort_index() # Sort by index (Vehicle ID)

plt.figure(figsize=(10,5))
vehicle_avg_duration.plot(kind='bar', color='steelblue')
plt.title('Average Delivery Duration per Vehicle (Sorted by ID)')
plt.xlabel('Vehicle ID')
plt.ylabel('Avg Delivery Duration (min)')
plt.xticks(rotation=45, ha='right') # Rotate labels for better readability
plt.tight_layout()
plt.show()

"""11.Outlier Route Detection(Z-score based)"""

from scipy.stats import zscore

df['duration_zscore'] = zscore(df['delivery_duration_mins'])
outliers = df[np.abs(df['duration_zscore']) > 3]

print("Number of outlier deliveries:", len(outliers))

"""12.Correlation with on-time delivery(Binary Target)"""

df['is_ontime'] = ((df['delivery_status'] >= -5) & (df['delivery_status'] <= 5)).astype(int)
corr = df.corr(numeric_only=True)['is_ontime'].sort_values(ascending=False)

plt.figure(figsize=(8,6))
sns.barplot(x=corr.values, y=corr.index, palette='coolwarm')
plt.title('Correlation with On-Time Delivery')
plt.xlabel('Correlation Coefficient')
plt.tight_layout()
plt.show()

df.info()

df.columns.tolist()

# prompt: compare if the delivery_duration_mins_scaled and duration_minutes_scaled are exactly same column or not?

# Compare the columns "delivery_duration_mins_scaled" and "duration_minutes_scaled"
# Check if the contents are exactly the same
are_scaled_columns_same = df['delivery_duration_mins_scaled'].equals(df['duration_minutes_scaled'])

if are_scaled_columns_same:
  print("Columns 'delivery_duration_mins_scaled' and 'duration_minutes_scaled' are identical.")
else:
  print("Columns 'delivery_duration_mins_scaled' and 'duration_minutes_scaled' are not identical.")

# prompt: boxplot of distance bin

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.boxplot(x='distance_bin', y='delivery_duration_mins', data=df, order=['0–2km', '2–5km', '5–10km', '10–20km', '20–50km', '50km+'])
plt.title("Delivery Duration by Distance Bin")
plt.xlabel("Distance Bin")
plt.ylabel("Delivery Duration (mins)")
plt.grid(axis='y')
plt.show()

# prompt: boxplot of urgency bin

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.boxplot(x='urgency_bin', y='delivery_duration_mins', data=df, order=['Overdue', '0–30m', '30–60m', '60–120m', '120m+'])
plt.title("Delivery Duration by Urgency Bin")
plt.xlabel("Urgency Bin")
plt.ylabel("Delivery Duration (mins)")
plt.grid(axis='y')
plt.show()

# prompt: boxplot of location cluster

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
sns.boxplot(x='location_cluster', y='delivery_duration_mins', data=df)
plt.title("Delivery Duration by Location Cluster")
plt.xlabel("Location Cluster")
plt.ylabel("Delivery Duration (mins)")
plt.grid(axis='y')
plt.show()

# prompt: boxplot of distance bin,urgency_bin and location cluster

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 8))
sns.boxplot(data=df, x='distance_bin', y='delivery_duration_mins', hue='urgency_bin', palette='viridis')
plt.title('Delivery Duration by Distance Bin and Urgency Bin')
plt.xlabel('Distance Bin')
plt.ylabel('Delivery Duration (mins)')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Urgency Bin')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 8))
sns.boxplot(data=df, x='location_cluster', y='delivery_duration_mins', palette='tab10')
plt.title('Delivery Duration by Location Cluster')
plt.xlabel('Location Cluster')
plt.ylabel('Delivery Duration (mins)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

plt.figure(figsize=(14, 8))
sns.boxplot(data=df, x='distance_bin', y='delivery_duration_mins', hue='location_cluster', palette='tab20')
plt.title('Delivery Duration by Distance Bin and Location Cluster')
plt.xlabel('Distance Bin')
plt.ylabel('Delivery Duration (mins)')
plt.xticks(rotation=45, ha='right')
plt.legend(title='Location Cluster')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""Pairplots"""

import seaborn as sns
import matplotlib.pyplot as plt

# Select relevant features
pairplot_features = ['delivery_duration_mins', 'distance_km', 'urgency_mins', 'duration_minutes', 'num_stops_before']

# Create pairplot
sns.pairplot(df[pairplot_features], diag_kind='kde', plot_kws={'alpha': 0.3})
plt.suptitle("Pairplot of Key Numerical Features vs Delivery Duration", y=1.02)
plt.show()

"""Multicollinearity"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

# Choose numerical features (you can modify based on your domain understanding)
vif_features = ['distance_km', 'urgency_mins', 'duration_minutes', 'route_duration_min', 'late_by_minutes', 'num_stops_before']

# Prepare data
X_vif = df[vif_features].dropna()  # ensure no NaNs
X_vif_const = add_constant(X_vif)

# Calculate VIF
vif_df = pd.DataFrame()
vif_df['Feature'] = X_vif.columns
vif_df['VIF'] = [variance_inflation_factor(X_vif_const.values, i+1) for i in range(len(X_vif.columns))]

print("Variance Inflation Factors (VIF):")
print(vif_df)

"""Quick Feature Importances"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

# Define features and target
rf_features = ['distance_km', 'urgency_mins', 'duration_minutes', 'route_duration_min', 'late_by_minutes', 'num_stops_before']
X = df[rf_features].dropna()
y = df.loc[X.index, 'delivery_duration_mins']

# Split and train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Feature importances
importances = pd.Series(rf.feature_importances_, index=rf_features).sort_values(ascending=False)

# Plot
importances.plot(kind='barh', title='Random Forest Feature Importances', figsize=(8, 5))
plt.gca().invert_yaxis()
plt.show()

# prompt: convert df to csv

df.to_csv('delivery_data.csv', index=False)

"""#ETA Regression Modelling

Baseline Model(Mean ETA)
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np

# Target variable
target = 'delivery_duration_mins'

# We'll use your finalized feature list later, for now only the target
y = df[target]

# Train-test split
_, X_test_baseline, _, y_test_baseline = train_test_split(df, y, test_size=0.2, random_state=42)

# Baseline prediction = mean of training data
baseline_pred = [y.mean()] * len(y_test_baseline)

# Evaluation
mae_baseline = mean_absolute_error(y_test_baseline, baseline_pred)
rmse_baseline = np.sqrt(mean_squared_error(y_test_baseline, baseline_pred))
mape_baseline = np.mean(np.abs((y_test_baseline - baseline_pred) / y_test_baseline)) * 100
r2_baseline = r2_score(y_test_baseline, baseline_pred)

print(f"📉 Baseline (Mean ETA) Model Performance:")
print(f"MAE  : {mae_baseline:.2f} minutes")
print(f"RMSE : {rmse_baseline:.2f} minutes")
print(f"MAPE : {mape_baseline:.2f}%")
print(f"R²   : {r2_baseline:.4f}")

"""Training Classsical Models(Linear Regression, Ridge Regression,Random Forest Regressor)"""

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd

# Final feature list from EDA + VIF + importance
features = [
    'distance_km',
    'urgency_mins',
    'route_duration_min',
    'num_stops_before',
    'late_by_minutes',
    'urgency_distance_interaction',
    'priority_distance_interaction',
    'weight_stops_interaction',
    'priority_level',
    'traffic_level_High', 'traffic_level_Medium', 'traffic_level_Low',
    'vehicle_type_EV', 'vehicle_type_GDV',
    'weather_condition_Clear', 'weather_condition_Fog',
    'weather_condition_Rain', 'weather_condition_Snow',
    'weight_category_encoded_label'
]

# Split features and target
X = df[features]
y = df['delivery_duration_mins']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)
}

# Train and evaluate
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    results[name] = {
        'MAE': mean_absolute_error(y_test, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),
        'R²': r2_score(y_test, y_pred)
    }

# Display results
results_df = pd.DataFrame(results).T
print("📊 Step 5.2: Classical Model Performance")
print(results_df.round(3))

"""XGBoost"""

from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
import pandas as pd

# Define features (same as used before)
features = [
    'distance_km',
    'urgency_mins',
    'route_duration_min',
    'num_stops_before',
    'late_by_minutes',
    'urgency_distance_interaction',
    'priority_distance_interaction',
    'weight_stops_interaction',
    'priority_level',
    'traffic_level_High', 'traffic_level_Medium', 'traffic_level_Low',
    'vehicle_type_EV', 'vehicle_type_GDV',
    'weather_condition_Clear', 'weather_condition_Fog',
    'weather_condition_Rain', 'weather_condition_Snow',
    'weight_category_encoded_label'
]

# Split data
X = df[features]
y = df['delivery_duration_mins']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train XGBoost
xgb_model = XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)
xgb_model.fit(X_train, y_train)

# Predict
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate
mae = mean_absolute_error(y_test, y_pred_xgb)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2 = r2_score(y_test, y_pred_xgb)

print("📊 XGBoost Model Performance:")
print(f"MAE  : {mae:.2f} minutes")
print(f"RMSE : {rmse:.2f} minutes")
print(f"R²   : {r2:.4f}")

"""Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error
import numpy as np

# Define feature matrix and target
X = df[features]  # same features list used earlier
y = df['delivery_duration_mins']

# ----------------------------
# 📌 Define custom scoring metrics
# ----------------------------
mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)
rmse_scorer = make_scorer(lambda y, y_pred: np.sqrt(mean_squared_error(y, y_pred)), greater_is_better=False)

# ----------------------------
# 🔍 Random Forest Tuning
# ----------------------------
rf_params = {
    'n_estimators': [50, 100],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}

rf = RandomForestRegressor(random_state=42)
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring=mae_scorer, n_jobs=-1, verbose=1)
rf_grid.fit(X, y)

print("✅ Best Random Forest Parameters:", rf_grid.best_params_)
print("📉 Best CV MAE:", -rf_grid.best_score_)

# ----------------------------
# ⚡️ XGBoost Tuning
# ----------------------------
xgb_params = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5],
    'learning_rate': [0.05, 0.1],
    'subsample': [0.8, 1.0]
}

xgb = XGBRegressor(random_state=42)
xgb_grid = GridSearchCV(xgb, xgb_params, cv=5, scoring=mae_scorer, n_jobs=-1, verbose=1)
xgb_grid.fit(X, y)

print("✅ Best XGBoost Parameters:", xgb_grid.best_params_)
print("📉 Best CV MAE:", -xgb_grid.best_score_)

"""Improved XGBoost and RF Regressor models"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
import numpy as np

# Features and target
X = df[features]
y = df['delivery_duration_mins']

# Split (reuse consistent random state for fairness)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ----------------------------
# ✅ Retrain Random Forest
# ----------------------------
best_rf = RandomForestRegressor(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42
)
best_rf.fit(X_train, y_train)
rf_preds = best_rf.predict(X_test)

# ----------------------------
# ✅ Retrain XGBoost
# ----------------------------
best_xgb = XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    subsample=0.8,
    random_state=42
)
best_xgb.fit(X_train, y_train)
xgb_preds = best_xgb.predict(X_test)

# ----------------------------
# 📊 Evaluate both on Test Set
# ----------------------------
def evaluate_model(y_true, y_pred, model_name):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"\n📋 {model_name} Test Set Performance")
    print(f"MAE  : {mae:.2f} minutes")
    print(f"RMSE : {rmse:.2f} minutes")
    print(f"R²   : {r2:.4f}")

# Evaluate both
evaluate_model(y_test, rf_preds, "Random Forest (Tuned)")
evaluate_model(y_test, xgb_preds, "XGBoost (Tuned)")

"""SHAP explanability for XGBoost"""

import shap
import matplotlib.pyplot as plt

# Only needed once
shap.initjs()

# SHAP explainer for tree-based model (XGBoost)
explainer = shap.TreeExplainer(best_xgb)
shap_values = explainer.shap_values(X_test)

# -------------------------------
# 📊 1. Global Feature Importance
# -------------------------------
shap.summary_plot(shap_values, X_test, plot_type="bar", show=True)

# -------------------------------
# 🐝 2. Bee Swarm Plot (summary)
# -------------------------------
shap.summary_plot(shap_values, X_test)

# -------------------------------
# 👁️ 3. Local Explanation (1 row)
# -------------------------------
# Choose an index to explain
row_index = 0  # change this to see other predictions
shap.force_plot(explainer.expected_value, shap_values[row_index], X_test.iloc[row_index], matplotlib=True)

"""SHAP explanability for RF Regressor"""

# ✅ Install if not already
!pip install shap

# ✅ Imports
import shap
import matplotlib.pyplot as plt

# ✅ Create explainer (SHAP auto-detects it's a tree-based model)
explainer_rf = shap.TreeExplainer(best_rf)

# ✅ Compute SHAP values on test data
shap_values_rf = explainer_rf.shap_values(X_test)

# ✅ Summary bar plot
shap.summary_plot(shap_values_rf, X_test, plot_type="bar", max_display=15)

# ✅ Summary dot plot (optional)
shap.summary_plot(shap_values_rf, X_test, max_display=15)

"""Residual Analysis on XGBoost"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# ✅ Predictions from your best XGBoost model
y_pred = best_xgb.predict(X_test)

# ✅ Residuals
residuals = y_test - y_pred

# 1. 📈 Predicted vs Actual
plt.figure(figsize=(7, 5))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.title('Predicted vs Actual Delivery Duration (mins)')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. 📉 Residuals vs Fitted
plt.figure(figsize=(7, 5))
sns.scatterplot(x=y_pred, y=residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Fitted (XGBoost)')
plt.xlabel('Predicted Delivery Duration')
plt.ylabel('Residuals (Actual - Predicted)')
plt.grid(True)
plt.tight_layout()
plt.show()

# 3. 📊 Histogram of Residuals
plt.figure(figsize=(7, 5))
sns.histplot(residuals, kde=True, bins=30)
plt.title('Histogram of Residuals')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Residual Analysis on RF Regressor"""

# ✅ Predictions from tuned Random Forest
rf_preds = best_rf.predict(X_test)

# ✅ Residuals for RF
rf_residuals = y_test - rf_preds

# 1. 📈 Predicted vs Actual (Random Forest)
plt.figure(figsize=(7, 5))
sns.scatterplot(x=y_test, y=rf_preds, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')
plt.title('Predicted vs Actual (Random Forest)')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.grid(True)
plt.tight_layout()
plt.show()

# 2. 📉 Residuals vs Fitted (Random Forest)
plt.figure(figsize=(7, 5))
sns.scatterplot(x=rf_preds, y=rf_residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Fitted (Random Forest)')
plt.xlabel('Predicted Delivery Duration')
plt.ylabel('Residuals')
plt.grid(True)
plt.tight_layout()
plt.show()

# 3. 📊 Histogram of Residuals (Random Forest)
plt.figure(figsize=(7, 5))
sns.histplot(rf_residuals, kde=True, bins=30)
plt.title('Histogram of Residuals (Random Forest)')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

# prompt: Can you run the linear and Ridge Regression on and match the naming convention as per next cell

import pandas as pd
import numpy as np
# Linear Regression Model
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lr_preds = lr_model.predict(X_test)

# Ridge Regression Model
ridge_model = Ridge(alpha=1.0) # Alpha is the regularization strength
ridge_model.fit(X_train, y_train)
ridge_preds = ridge_model.predict(X_test)

# Evaluation (using the same function as before)
evaluate_model(y_test, lr_preds, "Linear Regression")
evaluate_model(y_test, ridge_preds, "Ridge Regression")

# Update the results dictionary
results['Linear Regression'] = {
    'MAE': mean_absolute_error(y_test, lr_preds),
    'RMSE': np.sqrt(mean_squared_error(y_test, lr_preds)),
    'R²': r2_score(y_test, lr_preds)
}
results['Ridge Regression'] = {
    'MAE': mean_absolute_error(y_test, ridge_preds),
    'RMSE': np.sqrt(mean_squared_error(y_test, ridge_preds)),
    'R²': r2_score(y_test, ridge_preds)
}

# Display updated results
results_df = pd.DataFrame(results).T
print("\n📊 Step 5.2: Classical Model Performance (Updated)")
print(results_df.round(3))

# ✅ Residuals
lin_residuals = y_test - lr_preds
ridge_residuals = y_test - ridge_preds

# -----------------------------
# 📈 Predicted vs Actual (Linear)
# -----------------------------
plt.figure(figsize=(7, 5))
sns.scatterplot(x=y_test, y=lr_preds, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title('Predicted vs Actual (Linear Regression)')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.grid(True)
plt.tight_layout()
plt.show()

# 📉 Residuals vs Fitted (Linear)
plt.figure(figsize=(7, 5))
sns.scatterplot(x=lr_preds, y=lin_residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Fitted (Linear Regression)')
plt.xlabel('Predicted Delivery Duration')
plt.ylabel('Residuals')
plt.grid(True)
plt.tight_layout()
plt.show()

# 📊 Histogram of Residuals (Linear)
plt.figure(figsize=(7, 5))
sns.histplot(lin_residuals, kde=True, bins=30)
plt.title('Histogram of Residuals (Linear Regression)')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

# -----------------------------
# 📈 Predicted vs Actual (Ridge)
# -----------------------------
plt.figure(figsize=(7, 5))
sns.scatterplot(x=y_test, y=ridge_preds, alpha=0.6)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.title('Predicted vs Actual (Ridge Regression)')
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.grid(True)
plt.tight_layout()
plt.show()

# 📉 Residuals vs Fitted (Ridge)
plt.figure(figsize=(7, 5))
sns.scatterplot(x=ridge_preds, y=ridge_residuals, alpha=0.6)
plt.axhline(0, color='red', linestyle='--')
plt.title('Residuals vs Fitted (Ridge Regression)')
plt.xlabel('Predicted Delivery Duration')
plt.ylabel('Residuals')
plt.grid(True)
plt.tight_layout()
plt.show()

# 📊 Histogram of Residuals (Ridge)
plt.figure(figsize=(7, 5))
sns.histplot(ridge_residuals, kde=True, bins=30)
plt.title('Histogram of Residuals (Ridge Regression)')
plt.xlabel('Residuals')
plt.ylabel('Frequency')
plt.grid(True)
plt.tight_layout()
plt.show()

"""Residuals vs Key Features"""

import seaborn as sns
import matplotlib.pyplot as plt

for col in ['traffic_level', 'vehicle_type', 'weather_condition', 'weight_category']:
    plt.figure(figsize=(8, 5))
    sns.boxplot(x=df[col], y=residuals)
    plt.title(f'Residuals vs {col}')
    plt.axhline(0, color='red', linestyle='--')
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

"""Residuals by Distance Bucket"""

df['distance_bin'] = pd.cut(df['distance_km'], bins=[0, 2, 5, 10, 15], labels=['0–2', '2–5', '5–10', '10–15'])
plt.figure(figsize=(8,5))
sns.boxplot(x=df['distance_bin'], y=residuals)
plt.title('Residuals by Distance Bucket')
plt.axhline(0, color='red', linestyle='--')
plt.show()

"""Residual Statistics per Category"""

df['residuals'] = residuals

# Group residuals and get summary stats
grouped_stats = df.groupby('traffic_level')['residuals'].describe()
print("Residuals Summary by Traffic Level:\n", grouped_stats)

grouped_stats = df.groupby('vehicle_type')['residuals'].describe()
print("\nResiduals Summary by Vehicle Type:\n", grouped_stats)

grouped_stats = df.groupby('weather_condition')['residuals'].describe()
print("\nResiduals Summary by Weather Condition:\n", grouped_stats)

grouped_stats = df.groupby('weight_category')['residuals'].describe()
print("\nResiduals Summary by Weight Category:\n", grouped_stats)

grouped_stats = df.groupby('distance_bin')['residuals'].describe()
print("\nResiduals Summary by Distance Bin:\n", grouped_stats)

df.to_csv("final_dataframe.csv")

"""#Advanced Probabilistic Modelling"""

!pip install lightgbm

# prompt: convert csv to dataframe df

import pandas as pd
df = pd.read_csv('final_dataframe.csv')
df.head()

"""LightGBM Quantile Regression Forests"""

# ============================================================
# ✅ LightGBM Quantile Regression with Calibration Curve
# ============================================================

import lightgbm as lgb
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# --- Feature Selection
features_qrf = [
    'urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
    'route_duration_min', 'num_stops_before'
]
X = df[features_qrf]
y = df['delivery_duration_mins']

# --- Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Train 3 Quantile Models
models = {}
quantiles = [0.10, 0.5, 0.90]
for q in quantiles:
    model = lgb.LGBMRegressor(objective='quantile', alpha=q, learning_rate=0.1,
                               n_estimators=100, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    models[q] = model

# --- Predictions
pred_10 = models[0.10].predict(X_test)
pred_50 = models[0.5].predict(X_test)
pred_90 = models[0.90].predict(X_test)

# --- Evaluation
mae = mean_absolute_error(y_test, pred_50)
rmse = np.sqrt(mean_squared_error(y_test, pred_50))
r2 = r2_score(y_test, pred_50)
print("\n📊 LightGBM Quantile Regression")
print(f"MAE  : {mae:.2f} mins\nRMSE : {rmse:.2f} mins\nR2   : {r2:.4f}")

import joblib

joblib.dump(models[0.10], "model_10.pkl")
joblib.dump(models[0.50], "model_50.pkl")
joblib.dump(models[0.90], "model_90.pkl")

# prompt: create a csv file from final_dataframe using urgency_mins,late_by_minutes,duration_minutes,distance_km,route_duration_min,num_stops_before,delivery_lat,delivery_lon

df.to_csv('final_dataframe_route optimization.csv', index=False, columns=['urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km', 'route_duration_min', 'num_stops_before', 'delivery_lat', 'delivery_lon'])

import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split

# Load the dataset
df = pd.read_csv('/content/final_dataframe.csv')  # adjust path as needed

# Feature selection for ETA prediction
features_qrf = [
    'urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
    'route_duration_min', 'num_stops_before'
]
X = df[features_qrf]
y = df['delivery_duration_mins']

# Train/test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train quantile models
models = {}
quantiles = [0.10, 0.5, 0.90]
for q in quantiles:
    model = lgb.LGBMRegressor(objective='quantile', alpha=q, learning_rate=0.1,
                               n_estimators=100, max_depth=5, random_state=42)
    model.fit(X_train, y_train)
    models[q] = model

# Predict on test set
pred_10 = models[0.10].predict(X_test)
pred_50 = models[0.5].predict(X_test)
pred_90 = models[0.90].predict(X_test)

# Construct delivery_df with lat/lon + predictions
delivery_df = X_test.copy()
delivery_df['eta_10'] = pred_10
delivery_df['eta_50'] = pred_50
delivery_df['eta_90'] = pred_90

# Add lat/lon columns from original df
delivery_df['delivery_lat'] = df.loc[delivery_df.index, 'delivery_lat'].values
delivery_df['delivery_lon'] = df.loc[delivery_df.index, 'delivery_lon'].values

# Reset index for OR-Tools compatibility
delivery_df = delivery_df.reset_index(drop=True)

# Preview
print(delivery_df[['delivery_lat', 'delivery_lon', 'eta_10', 'eta_50', 'eta_90']].head(10))

delivery_df.to_csv("delivery_dataframe.csv")

import shap
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Import metrics used
import numpy as np # Import numpy if needed for calculations not shown here
import matplotlib.pyplot as plt # Import pyplot for plotting SHAP values

# --- SHAP Explainability for Predicted ETA (50th percentile)
# print("\n🔍 SHAP Explainability on LightGBM Median ETA")
# shap_explainer = shap.Explainer(models[0.5])
# shap_values = shap_explainer(X_test)
# shap.plots.bar(shap_values, max_display=10)
# shap.summary_plot(shap_values, X_test)
print("\n🔍 SHAP Explainability on LightGBM Median ETA")
shap_explainer = shap.Explainer(models[0.5])
shap_values = shap_explainer(X_test)
shap.plots.bar(shap_values, max_display=10)
shap.summary_plot(shap_values, X_test)

# Import GradientBoostingRegressor and plot_partial_dependence
from sklearn.ensemble import GradientBoostingRegressor
# Correct the import path for plot_partial_dependence
from sklearn.inspection import PartialDependenceDisplay # The function is now part of PartialDependenceDisplay

# --- Partial Dependence Plot (Top 3 Features)

# --- Partial Dependence Plot (Top 3 Features from SHAP)
print("\n📈 Partial Dependence Plots")
pdp_model = GradientBoostingRegressor().fit(X_train, y_train)
top_shap_idx = np.argsort(-np.abs(shap_values.values).mean(0))[:3]
top_shap_features = X_test.columns[top_shap_idx].tolist()
PartialDependenceDisplay.from_estimator(
    pdp_model,
    X_test,
    features=top_shap_features,
    grid_resolution=50
)
plt.tight_layout()
plt.show()

# --- Interval Width as Target for SHAP Analysis
print("\n🧠 SHAP on Interval Width (Uncertainty Attribution)")
interval_width = pred_90 - pred_10
interval_model = lgb.LGBMRegressor(n_estimators=100)
interval_model.fit(X_test, interval_width)
interval_explainer = shap.Explainer(interval_model)
interval_shap_values = interval_explainer(X_test)
shap.plots.bar(interval_shap_values, max_display=10)
shap.summary_plot(interval_shap_values, X_test)

# --- Interval Coverage
coverage = ((y_test >= pred_10) & (y_test <= pred_90)).mean()
print(f"Prediction Interval Coverage (10%-90%): {coverage*100:.2f}%")

# --- Pinball Loss for LightGBM
print("\n🔧 Pinball Loss (LightGBM):")
for alpha, preds in zip([0.10, 0.5, 0.90], [pred_10, pred_50, pred_90]):
    loss = np.mean(np.maximum(alpha * (y_test - preds), (alpha - 1) * (y_test - preds)))
    print(f"  α={alpha:.2f}: {loss:.4f}")

# --- Interval Width Distribution
plt.hist(interval_width, bins=30, color='orange', edgecolor='black')
plt.title("Interval Width Distribution (LightGBM)")
plt.xlabel("Width (minutes)")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Interval Width vs Distance
import seaborn as sns
plt.figure(figsize=(8, 5))
sns.scatterplot(x=X_test['distance_km'], y=interval_width)
plt.xlabel("Distance (km)")
plt.ylabel("Interval Width (minutes)")
plt.title("LightGBM: Interval Width vs Distance")
plt.grid(True)
plt.tight_layout()
plt.show()

# --- CALIBRATION FACTOR CURVE
factors = np.linspace(0.5, 2.0, 30)
actual_coverages = []
for factor in factors:
    lower = pred_50 - 0.5 * interval_width * factor
    upper = pred_50 + 0.5 * interval_width * factor
    cov = ((y_test >= lower) & (y_test <= upper)).mean()
    actual_coverages.append(cov * 100)

plt.figure(figsize=(10, 5))
plt.plot(factors, actual_coverages, marker='o')
plt.axhline(80, color='red', linestyle='--', label='Target 80%')
plt.xlabel("Calibration Factor")
plt.ylabel("Actual Interval Coverage (%)")
plt.title("Calibration Curve - LightGBM")
plt.grid(True)
plt.legend()
plt.show()

# Pick best factor near 80%
best_idx = np.argmin(np.abs(np.array(actual_coverages) - 80))
best_factor_lgb = factors[best_idx]
print(f"\n✅ Suggested Calibration Factor for LightGBM: {best_factor_lgb:.2f}")

# --- Calibrated Interval Evaluation
new_width = interval_width * best_factor_lgb
pred_calibrated_lower = pred_50 - 0.5 * new_width
pred_calibrated_upper = pred_50 + 0.5 * new_width
coverage_calibrated = np.mean((y_test >= pred_calibrated_lower) & (y_test <= pred_calibrated_upper))
print(f"✅ Calibrated Interval Coverage: {coverage_calibrated * 100:.2f}%")

# --- Calibrated Width Histogram
plt.figure(figsize=(7, 5))
plt.hist(new_width, bins=30, color='skyblue', edgecolor='black')
plt.title("Distribution of Calibrated Prediction Interval Widths")
plt.xlabel("Interval Width (minutes)")
plt.ylabel("Frequency")
plt.tight_layout()
plt.show()

# --- ETA vs Predicted with Calibrated Interval (100 samples)
n_samples = 100
sample_idx = np.random.choice(len(y_test), size=n_samples, replace=False)
actual_sample = y_test.iloc[sample_idx].values
pred_50_sample = pred_50[sample_idx]
pred_lower_sample = pred_calibrated_lower[sample_idx]
pred_upper_sample = pred_calibrated_upper[sample_idx]

plt.figure(figsize=(12, 6))
plt.plot(actual_sample, label="Actual ETA", marker='o')
plt.plot(pred_50_sample, label="Predicted Median ETA", marker='o')
plt.fill_between(
    np.arange(n_samples),
    pred_lower_sample,
    pred_upper_sample,
    color='lightblue',
    alpha=0.4,
    label='Calibrated 80% Interval'
)
plt.title("ETA Prediction with Calibrated Intervals (LightGBM)")
plt.xlabel("Sample Index")
plt.ylabel("ETA (minutes)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""NGBoost Modelling"""

!pip install ngboost lightgbm pandas scikit-learn matplotlib

import pandas as pd
df = pd.read_csv('final_dataframe.csv')

from scipy.stats import norm

def crps_gaussian(y, mu, sigma):
    """
    Computes the CRPS for Gaussian predictions.
    y: true values (array-like)
    mu: predicted means
    sigma: predicted std deviations
    """
    y = np.asarray(y)
    mu = np.asarray(mu)
    sigma = np.asarray(sigma)

    standardized = (y - mu) / sigma
    pdf = norm.pdf(standardized)
    cdf = norm.cdf(standardized)

    crps = sigma * (standardized * (2 * cdf - 1) + 2 * pdf - 1 / np.sqrt(np.pi))
    return np.mean(crps)

import numpy as np # numpy is needed for np.sqrt
from ngboost import NGBRegressor
from ngboost.distns import Normal
from ngboost.scores import CRPS
import seaborn as sns
from scipy.stats import norm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# ============================================================
# ✅ NGBoost with SHAP + PDP + Interval Explainability
# ============================================================

features_ngb = [
    'urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
    'route_duration_min', 'num_stops_before', 'priority_level', 'order_hour',
    'weather_condition_Clear', 'traffic_level_Medium', 'weight_category_encoded_label',
    'vehicle_type_EV', 'location_cluster', 'cluster_avg_duration'
]
X_ngb = df[features_ngb]
y_ngb = df['delivery_duration_mins']

X_train, X_test, y_train, y_test= train_test_split(X_ngb, y_ngb, test_size=0.2, random_state=42)

ngb = NGBRegressor(Dist=Normal, learning_rate=0.03, n_estimators=300, verbose=False)
ngb.fit(X_train, y_train)
pred_dist = ngb.pred_dist(X_test)
pred_mean = pred_dist.loc
pred_std = pred_dist.scale

mae = mean_absolute_error(y_test, pred_mean)
rmse = np.sqrt(mean_squared_error(y_test, pred_mean))
r2 = r2_score(y_test, pred_mean)
# Corrected line: Call the CRPS instance directly with the data
crps_value = crps_gaussian(y_test.values, pred_mean, pred_std)
print("\n📊 NGBoost")
print(f"MAE  : {mae:.2f} mins\nRMSE : {rmse:.2f} mins\nR2   : {r2:.4f}")
print(f"CRPS (manual): {crps_value:.4f}")

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ✅ Sample 100 rows to make KernelExplainer feasible in Colab
X_sample = X_test.sample(n=100, random_state=42)
X_bg = X_train.sample(n=100, random_state=0)  # background dataset

# ✅ Create a model-agnostic KernelExplainer for NGBoost mean predictions
explainer_ngb = shap.KernelExplainer(lambda X: ngb.pred_dist(X).loc, X_bg)

# ✅ Compute SHAP values on the sample test data
shap_values_ngb = explainer_ngb.shap_values(X_sample)

# ✅ Convert to SHAP format for plotting
shap_values_df = pd.DataFrame(shap_values_ngb, columns=X_sample.columns)

# ✅ SHAP Summary Plot
shap.summary_plot(shap_values_ngb, X_sample)

# ✅ SHAP Bar Plot (Top 10)
shap.plots.bar(shap.Explanation(values=shap_values_ngb, data=X_sample, feature_names=X_sample.columns), max_display=10)

# --- Partial Dependence Plots (Top 3 SHAP Features for NGBoost)
# Remove .values as shap_values_ngb is already a numpy array
top_ngb_shap_idx = np.argsort(-np.abs(shap_values_ngb.mean(0)))[:3]

# The following variables X_test_ngb, pdp_model_ngb, X_train_ngb, y_train_ngb are not defined in this cell
# and appear to be remnants from a previous attempt or a different notebook section.
# You should use the variables defined in ipython-input-15: X_test, X_train, y_train.
# Also, GradientBoostingRegressor and PartialDependenceDisplay are not imported.

# Import necessary libraries
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

# Use the correct test features DataFrame (X_test from ipython-input-15) to get column names
top_ngb_features = X_test.columns[top_ngb_shap_idx].tolist()

# Use the correct training data (X_train, y_train from ipython-input-15) for the PDP model
pdp_model_ngb = GradientBoostingRegressor(random_state=42).fit(X_train, y_train)

# Use the correct test features DataFrame (X_test from ipython-input-15) for plotting PDP
PartialDependenceDisplay.from_estimator(
    pdp_model_ngb,
    X_test,
    features=top_ngb_features,
    grid_resolution=50
)
plt.tight_layout()
plt.show()

import shap
import numpy as np
import pandas as pd
from sklearn.ensemble import GradientBoostingRegressor

# --- Compute interval width (~80% CI with z=1.28)
z = 1.28
interval_width_ngb = 2 * z * pred_std  # shape: (n_test,)

# --- Train surrogate model to predict interval width
surrogate_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
surrogate_model.fit(X_test, interval_width_ngb)

# --- SHAP Explainability on interval width
# Sample 100 points from test set
X_sample = X_test.sample(n=100, random_state=42)
X_bg = X_train.sample(n=100, random_state=0)

# Use KernelExplainer on the surrogate model
explainer_uncertainty = shap.KernelExplainer(surrogate_model.predict, X_bg)

# Compute SHAP values
shap_values_uncertainty = explainer_uncertainty.shap_values(X_sample)

# --- Plot SHAP summary and bar
shap.summary_plot(shap_values_uncertainty, X_sample)

# Optional: Bar Plot (Top 10)
shap.plots.bar(shap.Explanation(values=shap_values_uncertainty,
                                data=X_sample,
                                feature_names=X_sample.columns), max_display=10)

# --- Original Interval Coverage
z = 1.28
lower_ngb_orig = pred_mean - z * pred_std
upper_ngb_orig = pred_mean + z * pred_std
coverage_orig_ngb = np.mean((y_test >= lower_ngb_orig) & (y_test <= upper_ngb_orig))
print(f"📏 NGBoost Original 80% Interval Coverage: {coverage_orig_ngb * 100:.2f}%")

# --- Calibration Curve
factors = np.linspace(0.5, 2.0, 30)
actual_coverages_ngb = []
for factor in factors:
    lower = pred_mean - z * pred_std * factor
    upper = pred_mean + z * pred_std * factor
    coverage_ngb = ((y_test >= lower) & (y_test <= upper)).mean()
    actual_coverages_ngb.append(coverage_ngb * 100)

plt.figure(figsize=(10, 5))
plt.plot(factors, actual_coverages_ngb, marker='o')
plt.axhline(80, color='red', linestyle='--', label='Target 80%')
plt.xlabel("Calibration Factor")
plt.ylabel("Actual Interval Coverage (%)")
plt.title("Calibration Curve - NGBoost")
plt.grid(True)
plt.legend()
plt.show()

best_idx_ngb = np.argmin(np.abs(np.array(actual_coverages_ngb) - 80))
best_factor_ngb = factors[best_idx_ngb]
print(f"\n✅ Suggested Calibration Factor for NGBoost: {best_factor_ngb:.2f}")

# --- Final Calibrated Intervals
lower_ngb = pred_mean - z * pred_std * best_factor_ngb
upper_ngb = pred_mean + z * pred_std * best_factor_ngb
coverage_calib_ngb = np.mean((y_test >= lower_ngb) & (y_test <= upper_ngb))
print(f"✅ NGBoost Calibrated 80% Interval Coverage: {coverage_calib_ngb * 100:.2f}%")

# --- Original Interval Width Histogram
orig_interval_width = upper_ngb_orig - lower_ngb_orig
plt.figure(figsize=(7, 5))
plt.hist(orig_interval_width, bins=30, color='orange', edgecolor='black')
plt.title("NGBoost Original Prediction Interval Widths")
plt.xlabel("Interval Width (minutes)")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Calibrated Interval Width Histogram
calib_width = upper_ngb - lower_ngb
plt.figure(figsize=(7, 5))
plt.hist(calib_width, bins=30, color='skyblue', edgecolor='black')
plt.title("NGBoost Calibrated Prediction Interval Widths")
plt.xlabel("Interval Width (minutes)")
plt.ylabel("Frequency")
plt.grid(True)
plt.tight_layout()
plt.show()

# --- ETA vs Prediction Interval Plot (100 samples)
n_samples = 100
sample_idx = np.random.choice(len(y_test), size=n_samples, replace=False)
actual_sample = y_test.iloc[sample_idx].values
pred_mean_sample = pred_mean[sample_idx]
lower_sample = lower_ngb[sample_idx]
upper_sample = upper_ngb[sample_idx]

plt.figure(figsize=(12, 6))
plt.plot(actual_sample, label="Actual ETA", marker='o')
plt.plot(pred_mean_sample, label="Predicted Mean ETA", marker='o')
plt.fill_between(
    np.arange(n_samples),
    lower_sample,
    upper_sample,
    color='lightblue',
    alpha=0.4,
    label='Calibrated 80% Interval'
)
plt.title("ETA Prediction with Calibrated Intervals (NGBoost)")
plt.xlabel("Sample Index")
plt.ylabel("ETA (minutes)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# --- Distribution Overlay Plot (Optional Bonus)
x_vals = np.linspace(min(pred_mean)-30, max(pred_mean)+30, 300)
plt.figure(figsize=(12, 6))
for i in np.random.choice(len(pred_mean), 10, replace=False):
    mu = pred_mean[i]
    sigma = pred_std[i]
    sns.lineplot(x=x_vals, y=norm.pdf(x_vals, mu, sigma), label=f"Sample {i}", alpha=0.5)
plt.title("Predicted ETA Distributions for Random Test Samples (NGBoost)")
plt.xlabel("ETA (minutes)")
plt.ylabel("Probability Density")
plt.grid(True)
plt.tight_layout()
plt.show()

n_samples = 100
idx = np.random.choice(len(y_test), n_samples, replace=False)

plt.figure(figsize=(14, 6))
plt.errorbar(np.arange(n_samples), y_test.iloc[idx],
             yerr=pred_std[idx] * z * best_factor_ngb,
             fmt='o', label='Actual ± Predicted Interval', alpha=0.6, color='gray')
plt.plot(np.arange(n_samples), pred_mean[idx], 'r-', label='Predicted Mean')
plt.title("NGBoost: Predicted vs Actual ETA with Confidence Interval")
plt.xlabel("Sample Index")
plt.ylabel("ETA (minutes)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

"""#Route Optimization Incorporating ETA Uncertainty

Using LightGBM Model
"""

import pandas as pd
df = pd.read_csv('final_dataframe.csv')

!pip install ortools folium

pip install lightgbm

import pandas as pd
import joblib

df = pd.read_csv("final_dataframe_route optimization.csv")
model_10 = joblib.load("model_10.pkl")
model_50 = joblib.load("model_50.pkl")
model_90 = joblib.load("model_90.pkl")

features = ['urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
            'route_duration_min', 'num_stops_before']

df['eta_10'] = model_10.predict(df[features])
df['eta_50'] = model_50.predict(df[features])
df['eta_90'] = model_90.predict(df[features])

df.to_csv("updated_eta_df.csv", index=False)

import pandas as pd
df_eta = pd.read_csv("updated_eta_df.csv")

!pip install ortools folium geopy

from ortools.constraint_solver import pywrapcp, routing_enums_pb2
import pandas as pd
import numpy as np

# Load the already predicted ETA dataset
df = pd.read_csv('/content/updated_eta_df.csv')  # Make sure this has eta_50

# Use only 6 rows for feasibility
delivery_df = df.iloc[:6].copy().reset_index(drop=True)

# --- Create relaxed time windows
buffer = 30
delivery_df['time_window_start'] = (delivery_df['eta_50'] - buffer).clip(lower=0).astype(int)
delivery_df['time_window_end'] = (delivery_df['eta_50'] + buffer).astype(int)

time_windows = list(zip(delivery_df['time_window_start'], delivery_df['time_window_end']))

print("\n📦 ETA Time Windows :")
for i, (start, end) in enumerate(time_windows):
    print(f"Stop {i}: {start}–{end} min")

# --- Haversine travel time (assuming 30 km/h)
from math import radians, sin, cos, sqrt, atan2
def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

coords = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))
n = len(coords)
distance_matrix = np.zeros((n, n))
for i in range(n):
    for j in range(n):
        dist = haversine(*coords[i], *coords[j])
        distance_matrix[i][j] = int((dist / 30) * 60)  # km/h to minutes

# --- OR-Tools Setup
def create_data_model():
    return {
        'distance_matrix': distance_matrix.tolist(),
        'time_windows': time_windows,
        'num_vehicles': 1,
        'depot': 0
    }

data = create_data_model()
manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']), 1, 0)
routing = pywrapcp.RoutingModel(manager)

def distance_callback(from_index, to_index):
    return int(data['distance_matrix'][manager.IndexToNode(from_index)][manager.IndexToNode(to_index)])
transit_callback_index = routing.RegisterTransitCallback(distance_callback)
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

# Add time windows
routing.AddDimension(
    transit_callback_index,
    60,        # Allow waiting time (slack)
    1000,      # Max route duration
    False,
    'Time'
)
time_dim = routing.GetDimensionOrDie('Time')
for idx, window in enumerate(data['time_windows']):
    index = manager.NodeToIndex(idx)
    time_dim.CumulVar(index).SetRange(window[0], window[1])

# Solver settings
search_params = pywrapcp.DefaultRoutingSearchParameters()
search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC
search_params.time_limit.seconds = 10

# Solve
solution = routing.SolveWithParameters(search_params)

if solution:
    print("\n✅ Feasible Route Found:")
    index = routing.Start(0)
    route = []
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)
        route.append(node)
        index = solution.Value(routing.NextVar(index))
    route.append(manager.IndexToNode(index))
    print(" → ".join(map(str, route)))
else:
    print("❌ Still no feasible route. But this config should work — try running again or reduce to 4 stops.")

import folium
from folium.plugins import AntPath

# Coordinates of stops (index 0 is depot)
route_coords = [coords[i] for i in route]  # `coords` is already defined earlier from delivery_df

# Create Folium Map centered around depot
m = folium.Map(location=route_coords[0], zoom_start=12)

# Plot markers for each stop with ETA windows
for i, (lat, lon) in enumerate(route_coords[:-1]):  # Skip last depot repeat
    stop_idx = route[i]
    eta_range = f"{delivery_df.loc[stop_idx, 'time_window_start']}–{delivery_df.loc[stop_idx, 'time_window_end']} min"
    folium.Marker(
        location=[lat, lon],
        popup=f"Stop {stop_idx}<br>ETA: {eta_range}",
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color='blue', icon='truck', prefix='fa')
    ).add_to(m)

# Add depot with a different marker color
folium.Marker(
    location=route_coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Draw route using AntPath
AntPath(locations=route_coords, color='red', weight=4, dash_array=[10, 20]).add_to(m)
m.save("route_map_lgbm_5 stops.html")
print("Map saved to route_map_lgbm_5 stops.html")
# Show the map
m

# --- Only if solution exists
if solution:
    print("\n📊 ETA Summary with Uncertainty (LightGBM Route)\n")
    summary_data = []

    index = routing.Start(0)
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)

        if node != 0:  # Exclude depot
            eta_50 = delivery_df.iloc[node]['eta_50']
            eta_10 = delivery_df.iloc[node]['eta_10']
            eta_90 = delivery_df.iloc[node]['eta_90']
            uncertainty = (eta_90 - eta_10) / 2

            # Categorize risk level
            if uncertainty <= 5:
                risk = "Low"
            elif uncertainty <= 10:
                risk = "Medium"
            else:
                risk = "High"

            summary_data.append({
                "Stop": node,
                "ETA (Median)": f"{eta_50:.1f} min",
                "Uncertainty ±": f"{uncertainty:.1f} min",
                "Risk Level": risk
            })

        index = solution.Value(routing.NextVar(index))

    # Convert to DataFrame and display
    summary_df = pd.DataFrame(summary_data)
    display(summary_df)  # Works in Jupyter/Colab

import folium
from folium.plugins import AntPath

# Use delivery_df and summary_df from your working solution
# delivery_df should have delivery_lat, delivery_lon
# summary_df should have Stop, ETA (Median), Uncertainty ±, Risk Level

# Route from your OR-Tools output
route = [0, 4, 1, 2, 3, 5, 0]  # Depot + stops + return

# Get coordinates
coords = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))

# Map setup
m = folium.Map(location=coords[0], zoom_start=12)

# Add depot
folium.Marker(
    location=coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Risk color mapper
def risk_to_color(risk):
    return {
        "Low": "blue",
        "Medium": "orange",
        "High": "red"
    }.get(risk, "gray")

# Add stops using summary_df
for row in summary_df.itertuples():
    stop_idx = row.Stop
    eta = row._2         # ETA (Median)
    uncertainty = row._3 # Uncertainty ±
    risk = row._4        # Risk Level
    color = risk_to_color(risk)

    folium.Marker(
        location=[delivery_df.loc[stop_idx, 'delivery_lat'], delivery_df.loc[stop_idx, 'delivery_lon']],
        popup=(f"Stop {stop_idx}<br>ETA: {eta}<br>Uncertainty: {uncertainty}<br>Risk: {risk}"),
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color=color, icon='truck', prefix='fa')
    ).add_to(m)

# Draw route
route_coords = [coords[i] for i in route]
AntPath(route_coords, color="purple", weight=4).add_to(m)

# Save and display
m.save("final_risk_route_map_lightgbm_5stops.html")
print("✅ Map saved to: final_risk_route_map_lightgbm_5stops.html")
m

# --- LightGBM Route Duration Calculation
lgb_route_duration_5stops = 0
for i in range(len(route) - 1):
    lgb_route_duration_5stops += distance_matrix[route[i]][route[i+1]]

print(f"🕒 Total Route Duration (LightGBM): {lgb_route_duration_5stops} minutes")

results_5stops = []  # Define this at the top if not already
results_5stops.append({
    'Model': 'LightGBM',
    'Total Route Duration_5stops (min)': lgb_route_duration_5stops
})

from ortools.constraint_solver import pywrapcp, routing_enums_pb2
import pandas as pd
import numpy as np

# Load the already predicted ETA dataset
df = pd.read_csv('/content/updated_eta_df.csv')  # Make sure this has eta_50

# Use only 7 rows for feasibility
delivery_df = df.iloc[:7].copy().reset_index(drop=True)

# --- Create relaxed time windows
buffer = 40
delivery_df['time_window_start'] = (delivery_df['eta_50'] - buffer).clip(lower=0).astype(int)
delivery_df['time_window_end'] = (delivery_df['eta_50'] + buffer).astype(int)

time_windows = list(zip(delivery_df['time_window_start'], delivery_df['time_window_end']))

print("\n📦 ETA Time Windows :")
for i, (start, end) in enumerate(time_windows):
    print(f"Stop {i}: {start}–{end} min")

# --- Haversine travel time (assuming 30 km/h)
from math import radians, sin, cos, sqrt, atan2
def haversine(lat1, lon1, lat2, lon2):
    R = 6371
    dlat = radians(lat2 - lat1)
    dlon = radians(lon2 - lon1)
    a = sin(dlat/2)**2 + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1 - a))
    return R * c

coords = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))
n = len(coords)
distance_matrix = np.zeros((n, n))
for i in range(n):
    for j in range(n):
        dist = haversine(*coords[i], *coords[j])
        distance_matrix[i][j] = int((dist / 30) * 60)  # km/h to minutes

# --- OR-Tools Setup
def create_data_model():
    return {
        'distance_matrix': distance_matrix.tolist(),
        'time_windows': time_windows,
        'num_vehicles': 1,
        'depot': 0
    }

data = create_data_model()
manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']), 1, 0)
routing = pywrapcp.RoutingModel(manager)

def distance_callback(from_index, to_index):
    return int(data['distance_matrix'][manager.IndexToNode(from_index)][manager.IndexToNode(to_index)])
transit_callback_index = routing.RegisterTransitCallback(distance_callback)
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

# Add time windows
routing.AddDimension(
    transit_callback_index,
    60,        # Allow waiting time (slack)
    1000,      # Max route duration
    False,
    'Time'
)
time_dim = routing.GetDimensionOrDie('Time')
for idx, window in enumerate(data['time_windows']):
    index = manager.NodeToIndex(idx)
    time_dim.CumulVar(index).SetRange(window[0], window[1])

# Solver settings
search_params = pywrapcp.DefaultRoutingSearchParameters()
search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC
search_params.time_limit.seconds = 10

# Solve
solution = routing.SolveWithParameters(search_params)

if solution:
    print("\n✅ Feasible Route Found:")
    index = routing.Start(0)
    route = []
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)
        route.append(node)
        index = solution.Value(routing.NextVar(index))
    route.append(manager.IndexToNode(index))
    print(" → ".join(map(str, route)))
else:
    print("❌ Still no feasible route. But this config should work — try running again or reduce to 4 stops.")

import folium
from folium.plugins import AntPath

# Coordinates of stops (index 0 is depot)
route_coords = [coords[i] for i in route]  # `coords` is already defined earlier from delivery_df

# Create Folium Map centered around depot
m = folium.Map(location=route_coords[0], zoom_start=12)

# Plot markers for each stop with ETA windows
for i, (lat, lon) in enumerate(route_coords[:-1]):  # Skip last depot repeat
    stop_idx = route[i]
    eta_range = f"{delivery_df.loc[stop_idx, 'time_window_start']}–{delivery_df.loc[stop_idx, 'time_window_end']} min"
    folium.Marker(
        location=[lat, lon],
        popup=f"Stop {stop_idx}<br>ETA: {eta_range}",
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color='blue', icon='truck', prefix='fa')
    ).add_to(m)

# Add depot with a different marker color
folium.Marker(
    location=route_coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Draw route using AntPath
AntPath(locations=route_coords, color='red', weight=4, dash_array=[10, 20]).add_to(m)
m.save("route_map_lgbm_6 stops.html")
print("Map saved to route_map_lgbm_6 stops.html")
# Show the map
m

# --- Only if solution exists
if solution:
    print("\n📊 ETA Summary with Uncertainty (LightGBM Route)\n")
    summary_data = []

    index = routing.Start(0)
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)

        if node != 0:  # Exclude depot
            eta_50 = delivery_df.iloc[node]['eta_50']
            eta_10 = delivery_df.iloc[node]['eta_10']
            eta_90 = delivery_df.iloc[node]['eta_90']
            uncertainty = (eta_90 - eta_10) / 2

            # Categorize risk level
            if uncertainty <= 5:
                risk = "Low"
            elif uncertainty <= 10:
                risk = "Medium"
            else:
                risk = "High"

            summary_data.append({
                "Stop": node,
                "ETA (Median)": f"{eta_50:.1f} min",
                "Uncertainty ±": f"{uncertainty:.1f} min",
                "Risk Level": risk
            })

        index = solution.Value(routing.NextVar(index))

    # Convert to DataFrame and display
    summary_df = pd.DataFrame(summary_data)
    display(summary_df)  # Works in Jupyter/Colab

import folium
from folium.plugins import AntPath

# Use delivery_df and summary_df from your working solution
# delivery_df should have delivery_lat, delivery_lon
# summary_df should have Stop, ETA (Median), Uncertainty ±, Risk Level

# Route from your OR-Tools output
route = [0, 4, 1, 6, 2, 3, 5, 0]  # Depot + stops + return

# Get coordinates
coords = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))

# Map setup
m = folium.Map(location=coords[0], zoom_start=12)

# Add depot
folium.Marker(
    location=coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Risk color mapper
def risk_to_color(risk):
    return {
        "Low": "blue",
        "Medium": "orange",
        "High": "red"
    }.get(risk, "gray")

# Add stops using summary_df
for row in summary_df.itertuples():
    stop_idx = row.Stop
    eta = row._2         # ETA (Median)
    uncertainty = row._3 # Uncertainty ±
    risk = row._4        # Risk Level
    color = risk_to_color(risk)

    folium.Marker(
        location=[delivery_df.loc[stop_idx, 'delivery_lat'], delivery_df.loc[stop_idx, 'delivery_lon']],
        popup=(f"Stop {stop_idx}<br>ETA: {eta}<br>Uncertainty: {uncertainty}<br>Risk: {risk}"),
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color=color, icon='truck', prefix='fa')
    ).add_to(m)

# Draw route
route_coords = [coords[i] for i in route]
AntPath(route_coords, color="purple", weight=4).add_to(m)

# Save and display
m.save("final_risk_route_map_lightgbm_6stops.html")
print("✅ Map saved to: final_risk_route_map_lightgbm_6stops.html")
m

# --- LightGBM Route Duration Calculation
lgb_route_duration_6stops = 0
for i in range(len(route) - 1):
    lgb_route_duration_6stops += distance_matrix[route[i]][route[i+1]]

print(f"🕒 Total Route Duration for 6 stops (LightGBM): {lgb_route_duration_6stops} minutes")

results_6stops = []  # Define this at the top if not already
results_6stops.append({
    'Model': 'LightGBM',
    'Total Route Duration_6stops (min)': lgb_route_duration_6stops
})

"""NGBoost"""

!pip install ngboost folium ortools geopy

import pandas as pd
import numpy as np
import pickle
import folium
from geopy.distance import geodesic
from ortools.constraint_solver import pywrapcp, routing_enums_pb2
from ngboost import NGBRegressor

# --- Load dataset & model
df = pd.read_csv('/content/final_dataframe.csv')
with open("/content/ngboost_model.pkl", "rb") as f:
    ngb_model = pickle.load(f)

# --- Feature selection (must match training)
features_ngb = [
    'urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
    'route_duration_min', 'num_stops_before', 'priority_level', 'order_hour',
    'weather_condition_Clear', 'traffic_level_Medium', 'weight_category_encoded_label',
    'vehicle_type_EV', 'location_cluster', 'cluster_avg_duration'
]
X_ngb = df[features_ngb]

# --- Predict ETA
pred_dist = ngb_model.pred_dist(X_ngb)
df["eta_mean"] = pred_dist.loc
df["eta_std"] = pred_dist.scale
df["eta_lower"] = df["eta_mean"] - 1.28 * df["eta_std"]
df["eta_upper"] = df["eta_mean"] + 1.28 * df["eta_std"]

# --- Choose subset for feasibility
df = df.reset_index(drop=True)
delivery_df = df.loc[[1, 3, 5, 7, 9,11]].copy().reset_index(drop=True)

# --- Construct distance matrix using lat/lon
def haversine_distance(latlon1, latlon2):
    return geodesic(latlon1, latlon2).km

locations = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))
distance_matrix = [
    [int(haversine_distance(a, b) * 60 / 30) for b in locations]  # assume 30km/h
    for a in locations
]

# --- Define time windows with buffer
buffer = 25
time_windows = list(zip(
    (delivery_df["eta_lower"] - buffer).clip(lower=0).astype(int),
    (delivery_df["eta_upper"] + buffer).astype(int)
))
print("\n📦 ETA Time Windows (Buffered):")
for i, (l, u) in enumerate(time_windows):
    print(f"Stop {i}: {l}–{u} min")

# --- OR-Tools Data Model
def create_data_model():
    return {
        'distance_matrix': distance_matrix,
        'time_windows': time_windows,
        'num_vehicles': 1,
        'depot': 0,
    }

data = create_data_model()
manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']), data['num_vehicles'], data['depot'])
routing = pywrapcp.RoutingModel(manager)

# --- Callback for travel cost
def distance_callback(from_index, to_index):
    return data['distance_matrix'][manager.IndexToNode(from_index)][manager.IndexToNode(to_index)]
transit_callback_index = routing.RegisterTransitCallback(distance_callback)
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

# --- Time Window Constraints
routing.AddDimension(
    transit_callback_index,
    30,
    1000,
    False,
    'Time'
)
time_dimension = routing.GetDimensionOrDie('Time')
for idx, window in enumerate(data['time_windows']):
    index = manager.NodeToIndex(idx)
    time_dimension.CumulVar(index).SetRange(window[0], window[1])

# --- Solve the VRP
search_params = pywrapcp.DefaultRoutingSearchParameters()
search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC
solution = routing.SolveWithParameters(search_params)

# --- Output Route
if solution:
    route = []
    index = routing.Start(0)
    print("\n✅ Feasible Route Found:")
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)
        route.append(node)
        index = solution.Value(routing.NextVar(index))
    route.append(manager.IndexToNode(index))
    print(" → ".join(map(str, route)))

    # --- Map visualization
    route_coords = [locations[i] for i in route]
    m = folium.Map(location=route_coords[0], zoom_start=13)
    for idx, (lat, lon) in enumerate(route_coords[:-1]):
          stop_num = route[idx]
          label = f"Stop {stop_num}"

          if stop_num == 0:
        # Depot: Blue star icon
              folium.Marker(
                  location=(lat, lon),
                  popup="Depot (Start/End)",
                  icon=folium.Icon(color='green', icon='home', prefix='fa')
              ).add_to(m)
          else:
        # Delivery stops: Green circle
              folium.Marker(
                  location=(lat, lon),
                  popup=label,
                  icon=folium.Icon(color='blue', icon='truck',prefix='fa')
              ).add_to(m)



    AntPath(locations=route_coords, color='purple', weight=4, dash_array=[10, 20]).add_to(m)

    m.save("ngboost_feasible_route_map_5stops.html")
    print("\n🗺️ Route map saved as 'ngboost_feasible_route_map_5stops.html'")
else:
    print("❌ No feasible route found — increase buffer or reduce stops.")

m

from IPython.display import display
import pandas as pd

# We'll continue using your `delivery_df` that already includes:
# eta_mean, eta_std, and locations of stops 1,3,5,7,9,11

# Assign risk level
def compute_risk(std):
    if std <= 1.5:
        return "Low"
    elif std <= 3:
        return "Medium"
    else:
        return "High"

delivery_df["uncertainty"] = delivery_df["eta_std"]
delivery_df["risk_level"] = delivery_df["uncertainty"].apply(compute_risk)

# Prepare ETA summary table
summary_df = pd.DataFrame({
    "Stop": delivery_df.index,
    "ETA (Mean)": delivery_df["eta_mean"].round(1).astype(str) + " min",
    "Uncertainty ±": delivery_df["uncertainty"].round(1).astype(str) + " min",
    "Risk Level": delivery_df["risk_level"]
})

print("📊 ETA Summary with Uncertainty (NGBoost Route):")
display(summary_df)

import folium
from folium.plugins import AntPath

# Reuse route and delivery_df
route_coords = [locations[i] for i in route]

# Map
m = folium.Map(location=route_coords[0], zoom_start=12)

# Risk level → color
def get_marker_color(risk):
    return {
        "Low": "blue",
        "Medium": "orange",
        "High": "red"
    }.get(risk, "gray")

# Add depot (first stop)
folium.Marker(
    location=route_coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Add each stop with risk color
for i in range(1, len(route_coords) - 1):  # exclude depot repeats
    stop_idx = route[i]
    lat, lon = delivery_df.loc[stop_idx, ['delivery_lat', 'delivery_lon']]
    eta = delivery_df.loc[stop_idx, 'eta_mean']
    std = delivery_df.loc[stop_idx, 'uncertainty']
    risk = delivery_df.loc[stop_idx, 'risk_level']
    color = get_marker_color(risk)

    folium.Marker(
        location=[lat, lon],
        popup=f"Stop {stop_idx}<br>ETA: {eta:.1f} min<br>Uncertainty: ±{std:.1f} min<br>Risk: {risk}",
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color=color, icon='truck', prefix='fa')
    ).add_to(m)

# Draw the route
AntPath(route_coords, color="purple", weight=4).add_to(m)

# Save and display
m.save("ngboost_route_with_risk_map_5stops.html")
print("✅ Route map with risk saved as ngboost_route_with_risk_map_5stops.html")
m

# --- NGBoost Route Duration Calculation
ngb_route_duration_5stops = 0
for i in range(len(route) - 1):
    ngb_route_duration_5stops += distance_matrix[route[i]][route[i+1]]

print(f"🕒 Total Route Duration for 5 stops (NGBoost): {ngb_route_duration_5stops} minutes")

results_5stops.append({
    'Model': 'NGBoost',
    'Total Route Duration_5stops (min)': ngb_route_duration_5stops
})

import pandas as pd
import numpy as np
import pickle
import folium
from geopy.distance import geodesic
from ortools.constraint_solver import pywrapcp, routing_enums_pb2
from ngboost import NGBRegressor

# --- Load dataset & model
df = pd.read_csv('/content/final_dataframe.csv')
with open("/content/ngboost_model.pkl", "rb") as f:
    ngb_model = pickle.load(f)

# --- Feature selection (must match training)
features_ngb = [
    'urgency_mins', 'late_by_minutes', 'duration_minutes', 'distance_km',
    'route_duration_min', 'num_stops_before', 'priority_level', 'order_hour',
    'weather_condition_Clear', 'traffic_level_Medium', 'weight_category_encoded_label',
    'vehicle_type_EV', 'location_cluster', 'cluster_avg_duration'
]
X_ngb = df[features_ngb]

# --- Predict ETA
pred_dist = ngb_model.pred_dist(X_ngb)
df["eta_mean"] = pred_dist.loc
df["eta_std"] = pred_dist.scale
df["eta_lower"] = df["eta_mean"] - 1.28 * df["eta_std"]
df["eta_upper"] = df["eta_mean"] + 1.28 * df["eta_std"]

# --- Choose subset for feasibility
df = df.reset_index(drop=True)
delivery_df = df.loc[[1, 3, 5, 7, 9,11,13]].copy().reset_index(drop=True)

# --- Construct distance matrix using lat/lon
def haversine_distance(latlon1, latlon2):
    return geodesic(latlon1, latlon2).km

locations = list(zip(delivery_df['delivery_lat'], delivery_df['delivery_lon']))
distance_matrix = [
    [int(haversine_distance(a, b) * 60 / 30) for b in locations]  # assume 30km/h
    for a in locations
]

# --- Define time windows with buffer
buffer = 30
time_windows = list(zip(
    (delivery_df["eta_lower"] - buffer).clip(lower=0).astype(int),
    (delivery_df["eta_upper"] + buffer).astype(int)
))
print("\n📦 ETA Time Windows (Buffered):")
for i, (l, u) in enumerate(time_windows):
    print(f"Stop {i}: {l}–{u} min")

# --- OR-Tools Data Model
def create_data_model():
    return {
        'distance_matrix': distance_matrix,
        'time_windows': time_windows,
        'num_vehicles': 1,
        'depot': 0,
    }

data = create_data_model()
manager = pywrapcp.RoutingIndexManager(len(data['distance_matrix']), data['num_vehicles'], data['depot'])
routing = pywrapcp.RoutingModel(manager)

# --- Callback for travel cost
def distance_callback(from_index, to_index):
    return data['distance_matrix'][manager.IndexToNode(from_index)][manager.IndexToNode(to_index)]
transit_callback_index = routing.RegisterTransitCallback(distance_callback)
routing.SetArcCostEvaluatorOfAllVehicles(transit_callback_index)

# --- Time Window Constraints
routing.AddDimension(
    transit_callback_index,
    30,
    1000,
    False,
    'Time'
)
time_dimension = routing.GetDimensionOrDie('Time')
for idx, window in enumerate(data['time_windows']):
    index = manager.NodeToIndex(idx)
    time_dimension.CumulVar(index).SetRange(window[0], window[1])

# --- Solve the VRP
search_params = pywrapcp.DefaultRoutingSearchParameters()
search_params.first_solution_strategy = routing_enums_pb2.FirstSolutionStrategy.PATH_CHEAPEST_ARC
solution = routing.SolveWithParameters(search_params)

# --- Output Route
if solution:
    route = []
    index = routing.Start(0)
    print("\n✅ Feasible Route Found:")
    while not routing.IsEnd(index):
        node = manager.IndexToNode(index)
        route.append(node)
        index = solution.Value(routing.NextVar(index))
    route.append(manager.IndexToNode(index))
    print(" → ".join(map(str, route)))

    # --- Map visualization
    route_coords = [locations[i] for i in route]
    m = folium.Map(location=route_coords[0], zoom_start=13)
    for idx, (lat, lon) in enumerate(route_coords[:-1]):
          stop_num = route[idx]
          label = f"Stop {stop_num}"

          if stop_num == 0:
        # Depot: Blue star icon
              folium.Marker(
                  location=(lat, lon),
                  popup="Depot (Start/End)",
                  icon=folium.Icon(color='green', icon='home', prefix='fa')
              ).add_to(m)
          else:
        # Delivery stops: Green circle
              folium.Marker(
                  location=(lat, lon),
                  popup=label,
                  icon=folium.Icon(color='blue', icon='truck',prefix='fa')
              ).add_to(m)



    AntPath(locations=route_coords, color='purple', weight=4, dash_array=[10, 20]).add_to(m)
    m.save("ngboost_feasible_route_map_6stops.html")
    print("\n🗺️ Route map saved as 'ngboost_feasible_route_map_6stops.html'")
else:
    print("❌ No feasible route found — increase buffer or reduce stops.")

m

from IPython.display import display
import pandas as pd

# We'll continue using your `delivery_df` that already includes:
# eta_mean, eta_std, and locations of stops 1,3,5,7,9,11

# Assign risk level
def compute_risk(std):
    if std <= 1.5:
        return "Low"
    elif std <= 3:
        return "Medium"
    else:
        return "High"

delivery_df["uncertainty"] = delivery_df["eta_std"]
delivery_df["risk_level"] = delivery_df["uncertainty"].apply(compute_risk)

# Prepare ETA summary table
summary_df = pd.DataFrame({
    "Stop": delivery_df.index,
    "ETA (Mean)": delivery_df["eta_mean"].round(1).astype(str) + " min",
    "Uncertainty ±": delivery_df["uncertainty"].round(1).astype(str) + " min",
    "Risk Level": delivery_df["risk_level"]
})

print("📊 ETA Summary with Uncertainty (NGBoost Route):")
display(summary_df)

import folium
from folium.plugins import AntPath

# Reuse route and delivery_df
route_coords = [locations[i] for i in route]

# Map
m = folium.Map(location=route_coords[0], zoom_start=12)

# Risk level → color
def get_marker_color(risk):
    return {
        "Low": "blue",
        "Medium": "orange",
        "High": "red"
    }.get(risk, "gray")

# Add depot (first stop)
folium.Marker(
    location=route_coords[0],
    popup="Depot (Start/End)",
    tooltip="Depot",
    icon=folium.Icon(color='green', icon='home', prefix='fa')
).add_to(m)

# Add each stop with risk color
for i in range(1, len(route_coords) - 1):  # exclude depot repeats
    stop_idx = route[i]
    lat, lon = delivery_df.loc[stop_idx, ['delivery_lat', 'delivery_lon']]
    eta = delivery_df.loc[stop_idx, 'eta_mean']
    std = delivery_df.loc[stop_idx, 'uncertainty']
    risk = delivery_df.loc[stop_idx, 'risk_level']
    color = get_marker_color(risk)

    folium.Marker(
        location=[lat, lon],
        popup=f"Stop {stop_idx}<br>ETA: {eta:.1f} min<br>Uncertainty: ±{std:.1f} min<br>Risk: {risk}",
        tooltip=f"Stop {stop_idx}",
        icon=folium.Icon(color=color, icon='truck', prefix='fa')
    ).add_to(m)

# Draw the route
AntPath(route_coords, color="purple", weight=4).add_to(m)

# Save and display
m.save("ngboost_route_with_risk_map_6stops.html")
print("✅ Route map with risk saved as ngboost_route_with_risk_map_6stops.html")
m

# --- Comparison Table of Route Duration
summary_comparison_5stops = pd.DataFrame(results_5stops)
print("\n📊 Route Duration Comparison for 5 stops(LightGBM vs NGBoost)")
display(summary_comparison_5stops)  # use print(summary_comparison) if not in notebook

import matplotlib.pyplot as plt

# --- Bar chart of route duration
summary_comparison_5stops.plot(
    x='Model',
    y='Total Route Duration_5stops (min)',
    kind='bar',
    color=['#007acc', '#f57c00'],
    legend=False,
    title='Route Duration by Model'
)
plt.ylabel('Total Duration for 5 stops(minutes)')
plt.grid(axis='y')
plt.tight_layout()
plt.show()

